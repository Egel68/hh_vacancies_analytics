import asyncio
import json
from typing import List, Dict
from pathlib import Path
import pandas as pd

from getData import parse_vacancies_sync
from hh_parser_async import parse_vacancies_async
from processing import VacancyAnalyzer
from visualization import visualize_results


def analyze_single_query(query: str, vacancies: List[Dict],
                         base_output_dir: str = "./result") -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    safe_query = query.replace(' ', '_').replace('/', '_').lower()
    output_dir = Path(base_output_dir) / safe_query
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'=' * 60}")
    print(f"üìä –ê–Ω–∞–ª–∏–∑: {query}")
    print(f"{'=' * 60}")

    if not vacancies:
        print(f"‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è: {query}")
        return {}

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    with open(output_dir / 'raw.json', 'w', encoding='utf-8') as f:
        json.dump(vacancies, f, ensure_ascii=False, indent=2)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: raw.json")

    # –ê–Ω–∞–ª–∏–∑
    analyzer = VacancyAnalyzer(vacancies)
    df = analyzer.extract_data()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    df.to_csv(output_dir / 'processed.csv', index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: processed.csv ({len(df)} –≤–∞–∫–∞–Ω—Å–∏–π)")

    # –ù–∞–≤—ã–∫–∏
    skills_df = analyzer.analyze_skills()
    skills_df.to_csv(output_dir / 'skills.csv', index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: skills.csv")

    # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
    requirements_df = analyzer.analyze_requirements()
    requirements_df.to_csv(output_dir / 'requirements.csv', index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: requirements.csv")

    # –ó–∞—Ä–ø–ª–∞—Ç—ã
    salary_stats = analyzer.get_salary_stats()
    with open(output_dir / 'salary_stats.json', 'w', encoding='utf-8') as f:
        json.dump(salary_stats, f, ensure_ascii=False, indent=2)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: salary_stats.json")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\nüìà –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    visualize_results(analyzer, output_dir=str(output_dir), prefix="", show_plots=False)

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
    top_skills = skills_df.head(5)['–ù–∞–≤—ã–∫'].tolist() if len(skills_df) > 0 else []

    summary = {
        '–î–æ–ª–∂–Ω–æ—Å—Ç—å': query,
        '–í–∞–∫–∞–Ω—Å–∏–π': len(df),
        '–¢–æ–ø-5 –Ω–∞–≤—ã–∫–æ–≤': ', '.join(top_skills),
        '–°—Ä–µ–¥–Ω—è—è –ó–ü (–æ—Ç)': salary_stats.get('avg_from', 'N/A'),
        '–ú–µ–¥–∏–∞–Ω–∞ –ó–ü (–æ—Ç)': salary_stats.get('median_from', 'N/A'),
        '–ü–∞–ø–∫–∞': str(output_dir)
    }

    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è: {query}")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {output_dir}")

    return summary


def batch_analysis_sync(queries: List[str], area: int = 1,
                        max_vacancies: int = 100,
                        output_dir: str = "./result"):
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –°–ò–ù–•–†–û–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
    """
    print("=" * 60)
    print("üîÑ –ü–ê–ö–ï–¢–ù–´–ô –°–ò–ù–•–†–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ô")
    print("=" * 60)
    print(f"–î–æ–ª–∂–Ω–æ—Å—Ç–∏: {', '.join(queries)}\n")

    summary_list = []

    for query in queries:
        # –ü–∞—Ä—Å–∏–Ω–≥
        vacancies = parse_vacancies_sync(
            query=query,
            area=area,
            max_vacancies=max_vacancies,
            output_dir=output_dir
        )

        # –ê–Ω–∞–ª–∏–∑
        summary = analyze_single_query(query, vacancies, output_dir)
        if summary:
            summary_list.append(summary)

    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
    if summary_list:
        summary_df = pd.DataFrame(summary_list)
        summary_path = Path(output_dir) / 'batch_summary.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')

        print("\n" + "=" * 60)
        print("üìã –û–ë–©–ê–Ø –°–í–û–î–ö–ê")
        print("=" * 60)
        print(summary_df.to_string(index=False))
        print(f"\nüíæ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")

    print("\n‚úÖ –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


async def batch_analysis_async(queries: List[str], area: int = 1,
                               max_vacancies: int = 100,
                               max_concurrent: int = 10,
                               output_dir: str = "./result"):
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –ê–°–ò–ù–•–†–û–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
    """
    print("=" * 60)
    print("‚ö° –ü–ê–ö–ï–¢–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ô")
    print("=" * 60)
    print(f"–î–æ–ª–∂–Ω–æ—Å—Ç–∏: {', '.join(queries)}\n")

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä
    from hh_parser_async import HHParserAsync

    parser = HHParserAsync(max_concurrent_requests=max_concurrent, output_dir=output_dir)

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    tasks = []
    for query in queries:
        task = parser.parse_vacancies(query, area, max_vacancies)
        tasks.append(task)

    results = await asyncio.gather(*tasks)
    all_results = {query: result for query, result in zip(queries, results)}

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å
    summary_list = []

    for query, vacancies in all_results.items():
        summary = analyze_single_query(query, vacancies, output_dir)
        if summary:
            summary_list.append(summary)

    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
    if summary_list:
        summary_df = pd.DataFrame(summary_list)
        summary_path = Path(output_dir) / 'batch_summary.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')

        print("\n" + "=" * 60)
        print("üìã –û–ë–©–ê–Ø –°–í–û–î–ö–ê")
        print("=" * 60)
        print(summary_df.to_string(index=False))
        print(f"\nüíæ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")

    print("\n‚úÖ –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


def run_batch_analysis(queries: List[str], area: int = 1,
                       max_vacancies: int = 100,
                       async_mode: bool = True,
                       max_concurrent: int = 10,
                       output_dir: str = "./result"):
    """
    –ó–∞–ø—É—Å–∫ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏–ª–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
    """
    if async_mode:
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º
        try:
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        loop.run_until_complete(
            batch_analysis_async(queries, area, max_vacancies, max_concurrent, output_dir)
        )
    else:
        # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º
        batch_analysis_sync(queries, area, max_vacancies, output_dir)


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    queries = [
        'Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫',
        'Data Scientist',
        'Machine Learning Engineer'
    ]

    run_batch_analysis(
        queries=queries,
        area=1,
        max_vacancies=100,
        async_mode=True,  # True - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, False - —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        max_concurrent=8,
        output_dir="./result"
    )
