import asyncio
from typing import List, Dict
import json
from hh_parser_async import HHParserAsync
from processing import VacancyAnalyzer
import pandas as pd


async def parse_multiple_queries(queries: List[str], area: int = 1,
                                 max_vacancies_per_query: int = 100) -> Dict[str, List[Dict]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    """
    parser = HHParserAsync(max_concurrent_requests=15)

    tasks = []
    for query in queries:
        task = parser.parse_vacancies(query, area, max_vacancies_per_query)
        tasks.append(task)

    results = await asyncio.gather(*tasks)

    return {query: result for query, result in zip(queries, results)}


def batch_analysis(queries: List[str], area: int = 1, max_vacancies: int = 100):
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
    """
    print("=" * 60)
    print("–ü–ê–ö–ï–¢–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ô")
    print("=" * 60)
    print(f"–î–æ–ª–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {', '.join(queries)}\n")

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    loop = asyncio.get_event_loop()
    all_results = loop.run_until_complete(
        parse_multiple_queries(queries, area, max_vacancies)
    )

    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
    summary = []

    for query, vacancies in all_results.items():
        print(f"\n{'=' * 60}")
        print(f"–ê–Ω–∞–ª–∏–∑: {query}")
        print(f"{'=' * 60}")

        if not vacancies:
            print(f"‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è: {query}")
            continue

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        filename = query.replace(' ', '_').lower()
        with open(f'{filename}_raw.json', 'w', encoding='utf-8') as f:
            json.dump(vacancies, f, ensure_ascii=False, indent=2)

        # –ê–Ω–∞–ª–∏–∑
        analyzer = VacancyAnalyzer(vacancies)
        df = analyzer.extract_data()
        df.to_csv(f'{filename}_processed.csv', index=False, encoding='utf-8-sig')

        skills_df = analyzer.analyze_skills()
        skills_df.to_csv(f'{filename}_skills.csv', index=False, encoding='utf-8-sig')

        # –°–≤–æ–¥–∫–∞
        top_skills = skills_df.head(5)['–ù–∞–≤—ã–∫'].tolist()
        salary_stats = analyzer.get_salary_stats()

        summary_item = {
            '–î–æ–ª–∂–Ω–æ—Å—Ç—å': query,
            '–í–∞–∫–∞–Ω—Å–∏–π': len(df),
            '–¢–æ–ø-5 –Ω–∞–≤—ã–∫–æ–≤': ', '.join(top_skills),
            '–°—Ä–µ–¥–Ω—è—è –ó–ü (–æ—Ç)': salary_stats.get('avg_from', 'N/A'),
            '–ú–µ–¥–∏–∞–Ω–∞ –ó–ü (–æ—Ç)': salary_stats.get('median_from', 'N/A')
        }
        summary.append(summary_item)

        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(df)}")
        print(f"üìå –¢–æ–ø-5 –Ω–∞–≤—ã–∫–æ–≤: {', '.join(top_skills)}")

    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
    summary_df = pd.DataFrame(summary)
    summary_df.to_csv('batch_summary.csv', index=False, encoding='utf-8-sig')

    print("\n" + "=" * 60)
    print("–û–ë–©–ê–Ø –°–í–û–î–ö–ê")
    print("=" * 60)
    print(summary_df.to_string(index=False))
    print("\n‚úÖ –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    queries = [
        'Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫',
        'Data Scientist',
        'Machine Learning Engineer',
        'Backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫'
    ]

    batch_analysis(queries, area=1, max_vacancies=100)