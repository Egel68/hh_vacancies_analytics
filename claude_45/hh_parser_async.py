import aiohttp
import asyncio
from typing import List, Dict, Optional
import json
from aiohttp import ClientSession, TCPConnector
import time
from pathlib import Path


class HHParserAsync:
    def __init__(self, max_concurrent_requests: int = 10, output_dir: str = "./result"):
        """
        max_concurrent_requests: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        output_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.base_url = "https://api.hh.ru/vacancies"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'HH-User-Agent': 'VacancyParser/1.0'
        }
        self.max_concurrent_requests = max_concurrent_requests
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.request_count = 0
        self.error_count = 0

    async def fetch(self, session: ClientSession, url: str, params: dict = None,
                    retry_count: int = 3, retry_delay: float = 1.0) -> Optional[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        async with self.semaphore:
            for attempt in range(retry_count):
                try:
                    self.request_count += 1

                    async with session.get(url, params=params, headers=self.headers,
                                           timeout=aiohttp.ClientTimeout(total=30)) as response:

                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ rate limiting
                        if response.status == 429:
                            retry_after = int(response.headers.get('Retry-After', 60))
                            print(f"‚ö†Ô∏è  Rate limit! –ñ–¥–µ–º {retry_after} —Å–µ–∫—É–Ω–¥...")
                            await asyncio.sleep(retry_after)
                            continue

                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
                        if response.status == 403:
                            print(f"‚ö†Ô∏è  403 Forbidden –¥–ª—è {url}")
                            self.error_count += 1
                            await asyncio.sleep(retry_delay * (attempt + 1))
                            continue

                        if response.status == 400:
                            # 400 –æ–±—ã—á–Ω–æ –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                            print(f"‚ö†Ô∏è  400 Bad Request –¥–ª—è {url} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {params}")
                            return None

                        if response.status == 404:
                            # –í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∞
                            return None

                        response.raise_for_status()
                        data = await response.json()

                        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        await asyncio.sleep(0.1)

                        return data

                except aiohttp.ClientError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}/{retry_count}: {e}")
                    self.error_count += 1

                    if attempt < retry_count - 1:
                        delay = retry_delay * (2 ** attempt)  # Exponential backoff
                        await asyncio.sleep(delay)
                    else:
                        return None

                except asyncio.TimeoutError:
                    print(f"‚ö†Ô∏è  Timeout –¥–ª—è {url}")
                    if attempt < retry_count - 1:
                        await asyncio.sleep(retry_delay * (attempt + 1))
                    else:
                        return None

                except Exception as e:
                    print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                    return None

            return None

    async def search_vacancies_page(self, session: ClientSession, query: str,
                                    area: int, page: int) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ pages)"""
        params = {
            'text': query,
            'area': area,
            'page': page,
            'per_page': 100
        }

        return await self.fetch(session, self.base_url, params)

    async def search_all_vacancies(self, query: str, area: int = 1,
                                   max_pages: int = 20) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º"""
        print(f"üîç –ò—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏: {query}")

        connector = TCPConnector(limit=30, limit_per_host=10, force_close=False)
        timeout = aiohttp.ClientTimeout(total=300, connect=60)

        async with ClientSession(connector=connector, timeout=timeout) as session:
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            first_response = await self.search_vacancies_page(session, query, area, 0)

            if not first_response or 'items' not in first_response:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏")
                return []

            all_vacancies = first_response['items']
            total_pages = min(first_response.get('pages', 1), max_pages, 20)  # API hh.ru –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç 20 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
            total_found = first_response.get('found', 0)

            print(f"üìä –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {total_found}")
            print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_pages}")

            if total_pages <= 1:
                return all_vacancies

            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            tasks = []
            for page in range(1, total_pages):
                task = self.search_vacancies_page(session, query, area, page)
                tasks.append(task)

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–∞–∫–µ—Ç–∞–º–∏
            batch_size = 5
            for i in range(0, len(tasks), batch_size):
                batch = tasks[i:i + batch_size]
                results = await asyncio.gather(*batch)

                for result in results:
                    if result and 'items' in result:
                        all_vacancies.extend(result['items'])

                print(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(all_vacancies)}")

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
                if i + batch_size < len(tasks):
                    await asyncio.sleep(0.5)

            print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(all_vacancies)}")
            return all_vacancies

    async def get_vacancy_details(self, session: ClientSession,
                                  vacancy_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        url = f"https://api.hh.ru/vacancies/{vacancy_id}"
        return await self.fetch(session, url)

    async def get_vacancies_details_batch(self, vacancy_ids: List[str],
                                          batch_size: int = 20) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö –ø–∞–∫–µ—Ç–∞–º–∏"""
        all_details = []
        total = len(vacancy_ids)

        print(f"\nüìã –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è {total} –≤–∞–∫–∞–Ω—Å–∏–π...")

        connector = TCPConnector(limit=30, limit_per_host=10, force_close=False)
        timeout = aiohttp.ClientTimeout(total=300, connect=60)

        async with ClientSession(connector=connector, timeout=timeout) as session:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞–∫–µ—Ç—ã
            for i in range(0, len(vacancy_ids), batch_size):
                batch = vacancy_ids[i:i + batch_size]

                tasks = [
                    self.get_vacancy_details(session, vac_id)
                    for vac_id in batch
                ]

                results = await asyncio.gather(*tasks)

                # –§–∏–ª—å—Ç—Ä—É–µ–º None
                batch_details = [r for r in results if r is not None]
                all_details.extend(batch_details)

                processed = min(i + batch_size, total)
                percentage = processed / total * 100
                print(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}/{total} ({percentage:.1f}%)")

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
                if i + batch_size < len(vacancy_ids):
                    await asyncio.sleep(1.0)

        return all_details

    async def parse_vacancies(self, query: str, area: int = 1,
                              max_vacancies: int = 100) -> List[Dict]:
        """–ü–æ–ª–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π"""
        start_time = time.time()

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π
        vacancies_list = await self.search_all_vacancies(query, area, max_pages=10)

        if not vacancies_list:
            print("‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        vacancies_list = vacancies_list[:max_vacancies]

        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        vacancy_ids = [v['id'] for v in vacancies_list]
        detailed_vacancies = await self.get_vacancies_details_batch(vacancy_ids, batch_size=20)

        elapsed_time = time.time() - start_time

        print(f"\n{'=' * 60}")
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(detailed_vacancies)} –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ {len(vacancy_ids)}")
        print(f"üìà –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {self.request_count}")
        print(f"‚ö†Ô∏è  –û—à–∏–±–æ–∫: {self.error_count}")
        print(f"{'=' * 60}\n")

        return detailed_vacancies

    def save_to_json(self, data: any, filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞
def parse_vacancies_async(query: str, area: int = 1,
                          max_vacancies: int = 100,
                          max_concurrent: int = 10,
                          output_dir: str = "./result") -> List[Dict]:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
    """
    parser = HHParserAsync(max_concurrent_requests=max_concurrent, output_dir=output_dir)

    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º event loop
    try:
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    vacancies = loop.run_until_complete(
        parser.parse_vacancies(query, area, max_vacancies)
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if vacancies:
        safe_query = query.replace(' ', '_').replace('/', '_').lower()
        parser.save_to_json(vacancies, f'{safe_query}_raw.json')

    return vacancies
