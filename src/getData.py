"""
–ú–æ–¥—É–ª—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å hh.ru API.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–ª–∞—Å—Å HHParser –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤.
"""

import requests
import time
import json
from typing import List, Dict, Optional
from pathlib import Path


class HHParser:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π —Å HeadHunter API.

    Attributes:
        base_url: –ë–∞–∑–æ–≤—ã–π URL API HH.ru
        headers: HTTP-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """

    def __init__(self, output_dir: str = "./result"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–µ—Ä.

        Args:
            output_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.base_url = "https://api.hh.ru/vacancies"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        }
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def search_vacancies(
            self,
            query: str,
            area: int = 1,
            pages: int = 20
    ) -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏)
            area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±, 113 - –†–æ—Å—Å–∏—è)
            pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫—Ä–∞—Ç–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
        """
        vacancies = []

        for page in range(pages):
            params = {
                'text': query,
                'area': area,
                'page': page,
                'per_page': 100
            }

            try:
                response = requests.get(self.base_url, params=params, headers=self.headers)
                response.raise_for_status()
                data = response.json()

                if 'items' not in data:
                    break

                vacancies.extend(data['items'])
                print(f"üì• –°–æ–±—Ä–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(vacancies)}")

                if page >= data['pages'] - 1:
                    break

                time.sleep(0.5)

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
                break

        return vacancies

    def get_vacancy_details(self, vacancy_id: str) -> Optional[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏.

        Args:
            vacancy_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–∞–∫–∞–Ω—Å–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        url = f"https://api.hh.ru/vacancies/{vacancy_id}"

        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}: {e}")
            return None

    def parse_vacancies(
            self,
            query: str,
            area: int = 1,
            max_vacancies: int = 100
    ) -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏)
            area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±, 113 - –†–æ—Å—Å–∏—è)
            max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
        """
        print(f"üîç –ò—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏: {query}")

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π
        vacancies_list = self.search_vacancies(query, area, pages=10)
        vacancies_list = vacancies_list[:max_vacancies]

        print(f"\nüìã –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...")
        detailed_vacancies = []

        for i, vacancy in enumerate(vacancies_list, 1):
            details = self.get_vacancy_details(vacancy['id'])
            if details:
                detailed_vacancies.append(details)
                print(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(vacancies_list)} ({i / len(vacancies_list) * 100:.1f}%)")
                time.sleep(0.2)

        print(f"\n‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(detailed_vacancies)} –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π")
        return detailed_vacancies

    def save_to_json(self, data: any, filename: str) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON-—Ñ–∞–π–ª.

        Args:
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")


def parse_vacancies_sync(
    query: str,
    area: int = 1,
    max_vacancies: int = 100,
    output_dir: str = "./result",
    save_raw: bool = True
) -> List[Dict]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏)
        area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±, 113 - –†–æ—Å—Å–∏—è)
        max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_raw: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ raw.json —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
    """
    parser = HHParser(output_dir=output_dir)
    vacancies = parser.parse_vacancies(query, area, max_vacancies)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ save_raw=True
    if vacancies and save_raw:
        safe_query = query.replace(' ', '_').replace('/', '_').lower()
        parser.save_to_json(vacancies, f'{safe_query}_raw.json')

    return vacancies
