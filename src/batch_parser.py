"""
–ú–æ–¥—É–ª—å –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å hh.ru.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
"""

import asyncio
import json
from typing import List, Dict, Optional
from pathlib import Path
import pandas as pd
from getData import parse_vacancies_sync
from processing import VacancyAnalyzer
from visualization import visualize_results


def analyze_single_query(
        query: str,
        vacancies: List[Dict],
        base_output_dir: str = "./result"
) -> Dict:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –æ–¥–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏.

    Args:
        query: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        vacancies: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –≤–∞–∫–∞–Ω—Å–∏–π
        base_output_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∞–Ω–∞–ª–∏–∑–∞, –≤–∫–ª—é—á–∞—é—â–∏–π:
            - –î–æ–ª–∂–Ω–æ—Å—Ç—å
            - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π
            - –¢–æ–ø-5 –Ω–∞–≤—ã–∫–æ–≤
            - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º
            - –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    safe_query = query.replace(' ', '_').replace('/', '_').lower()
    output_dir = Path(base_output_dir) / safe_query
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'=' * 60}")
    print(f"üìä –ê–Ω–∞–ª–∏–∑: {query}")
    print(f"{'=' * 60}")

    if not vacancies:
        print(f"‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è: {query}")
        return {}

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    with open(output_dir / 'raw.json', 'w', encoding='utf-8') as f:
        json.dump(vacancies, f, ensure_ascii=False, indent=2)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: raw.json")

    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    analyzer = VacancyAnalyzer(vacancies)
    df = analyzer.extract_data()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    df.to_csv(output_dir / 'processed.csv', index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: processed.csv ({len(df)} –≤–∞–∫–∞–Ω—Å–∏–π)")

    # –ê–Ω–∞–ª–∏–∑ –Ω–∞–≤—ã–∫–æ–≤
    skills_df = analyzer.analyze_skills()
    skills_df.to_csv(output_dir / 'skills.csv', index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: skills.csv")

    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    requirements_df = analyzer.analyze_requirements()
    requirements_df.to_csv(output_dir / 'requirements.csv', index=False, encoding='utf-8-sig')
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: requirements.csv")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º
    salary_stats = analyzer.get_salary_stats()
    with open(output_dir / 'salary_stats.json', 'w', encoding='utf-8') as f:
        json.dump(salary_stats, f, ensure_ascii=False, indent=2)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: salary_stats.json")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\nüìà –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    visualize_results(analyzer, output_dir=str(output_dir), prefix="", show_plots=False)

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
    top_skills = skills_df.head(5)['–ù–∞–≤—ã–∫'].tolist() if len(skills_df) > 0 else []

    summary = {
        '–î–æ–ª–∂–Ω–æ—Å—Ç—å': query,
        '–í–∞–∫–∞–Ω—Å–∏–π': len(df),
        '–¢–æ–ø-5 –Ω–∞–≤—ã–∫–æ–≤': ', '.join(top_skills),
        '–°—Ä–µ–¥–Ω—è—è –ó–ü (–æ—Ç)': salary_stats.get('avg_from', 'N/A'),
        '–ú–µ–¥–∏–∞–Ω–∞ –ó–ü (–æ—Ç)': salary_stats.get('median_from', 'N/A'),
        '–ü–∞–ø–∫–∞': str(output_dir)
    }

    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è: {query}")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {output_dir}")

    return summary


def batch_analysis_sync(
    queries: List[str],
    area: int = 1,
    max_vacancies: int = 100,
    output_dir: str = "./result"
) -> None:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π.

    Args:
        queries: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ –ø–æ–∏—Å–∫–∞ (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±, 113 - –†–æ—Å—Å–∏—è)
        max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    print("=" * 60)
    print("üîÑ –ü–ê–ö–ï–¢–ù–´–ô –°–ò–ù–•–†–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ô")
    print("=" * 60)
    print(f"–î–æ–ª–∂–Ω–æ—Å—Ç–∏: {', '.join(queries)}\n")

    summary_list = []

    for query in queries:
        # –ü–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π (–ë–ï–ó —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è raw —Ñ–∞–π–ª–∞)
        vacancies = parse_vacancies_sync(
            query=query,
            area=area,
            max_vacancies=max_vacancies,
            output_dir=output_dir,
            save_raw=False  # <- –û—Ç–∫–ª—é—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑–æ–≤—É—é –ø–∞–ø–∫—É
        )

        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (raw.json —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–æ–¥–ø–∞–ø–∫—É)
        summary = analyze_single_query(query, vacancies, output_dir)
        if summary:
            summary_list.append(summary)

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–π —Å–≤–æ–¥–∫–∏
    if summary_list:
        summary_df = pd.DataFrame(summary_list)
        summary_path = Path(output_dir) / 'batch_summary.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')

        print("\n" + "=" * 60)
        print("üìã –û–ë–©–ê–Ø –°–í–û–î–ö–ê")
        print("=" * 60)
        print(summary_df.to_string(index=False))
        print(f"\nüíæ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")

    print("\n‚úÖ –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


async def batch_analysis_async(
    queries: List[str],
    area: int = 1,
    max_vacancies: int = 100,
    max_concurrent: int = 10,
    output_dir: str = "./result"
) -> None:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π.

    Args:
        queries: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ –ø–æ–∏—Å–∫–∞ (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±, 113 - –†–æ—Å—Å–∏—è)
        max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    print("=" * 60)
    print("‚ö° –ü–ê–ö–ï–¢–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –í–ê–ö–ê–ù–°–ò–ô")
    print("=" * 60)
    print(f"–î–æ–ª–∂–Ω–æ—Å—Ç–∏: {', '.join(queries)}\n")

    from hh_parser_async import HHParserAsync

    parser = HHParserAsync(max_concurrent_requests=max_concurrent, output_dir=output_dir)

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π (raw —Ñ–∞–π–ª—ã –ù–ï —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    tasks = [parser.parse_vacancies(query, area, max_vacancies) for query in queries]
    results = await asyncio.gather(*tasks)
    all_results = dict(zip(queries, results))

    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ (raw.json —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–æ–¥–ø–∞–ø–∫—É)
    summary_list = []
    for query, vacancies in all_results.items():
        summary = analyze_single_query(query, vacancies, output_dir)
        if summary:
            summary_list.append(summary)

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–π —Å–≤–æ–¥–∫–∏
    if summary_list:
        summary_df = pd.DataFrame(summary_list)
        summary_path = Path(output_dir) / 'batch_summary.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')

        print("\n" + "=" * 60)
        print("üìã –û–ë–©–ê–Ø –°–í–û–î–ö–ê")
        print("=" * 60)
        print(summary_df.to_string(index=False))
        print(f"\nüíæ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")

    print("\n‚úÖ –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


def run_batch_analysis(
        queries: List[str],
        area: int = 1,
        max_vacancies: int = 100,
        async_mode: bool = True,
        max_concurrent: int = 10,
        output_dir: str = "./result"
) -> None:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∏–ª–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.

    Args:
        queries: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ –ø–æ–∏—Å–∫–∞ (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±, 113 - –†–æ—Å—Å–∏—è)
        max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        async_mode: –ï—Å–ª–∏ True - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º, –∏–Ω–∞—á–µ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–¥–ª—è async)
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if async_mode:
        try:
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        loop.run_until_complete(
            batch_analysis_async(queries, area, max_vacancies, max_concurrent, output_dir)
        )
    else:
        batch_analysis_sync(queries, area, max_vacancies, output_dir)


if __name__ == "__main__":
    queries = [
        'Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫',
        'Data Scientist',
        'Machine Learning Engineer'
    ]

    run_batch_analysis(
        queries=queries,
        area=1,
        max_vacancies=100,
        async_mode=True,
        max_concurrent=8,
        output_dir="./result"
    )
