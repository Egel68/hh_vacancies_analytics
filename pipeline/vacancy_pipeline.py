"""
–ú–æ–¥—É–ª—å –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–π.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã (Facade pattern).
"""

import json
from pathlib import Path
from typing import List, Dict, Optional
import pandas as pd

from config import Config  # ‚Üê –ü–ï–†–ï–ù–ï–°–ï–ù –í –ù–ê–ß–ê–õ–û
from core.interfaces import (
    IVacancySearcher,
    IVacancyDetailsFetcher,
    IVacancyAnalyzer,
    IVacancyVisualizer,
    IDescriptionProcessor
)
from storage.savers import JsonSaver, CsvSaver
from parsers.text_cleaner import HtmlTextCleaner
from extractors.requirements_extractor import (
    RequirementsExtractor,
    SkillsBasedRequirementsExtractor
)
from extractors.responsibilities_extractor import ResponsibilitiesExtractor
from processors.description_processor import VacancyDescriptionProcessor


class VacancyPipeline:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–π.

    –†–µ–∞–ª–∏–∑—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω Facade –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å —Å–∏—Å—Ç–µ–º–æ–π.
    –°–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É Open/Closed - –æ—Ç–∫—Ä—ã—Ç –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ DI.
    –°–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É Dependency Inversion - –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤.
    """

    def __init__(
            self,
            searcher: IVacancySearcher,
            details_fetcher: IVacancyDetailsFetcher,
            analyzer_class: type,
            visualizer: IVacancyVisualizer,
            output_dir: str = "./result"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pipeline.

        Args:
            searcher: –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π
            details_fetcher: –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π
            analyzer_class: –ö–ª–∞—Å—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞)
            visualizer: –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            output_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.searcher = searcher
        self.details_fetcher = details_fetcher
        self.analyzer_class = analyzer_class
        self.visualizer = visualizer
        self.output_dir = Path(output_dir)

        self.json_saver = JsonSaver()
        self.csv_saver = CsvSaver()

    def process_single_query(
            self,
            query: str,
            area: int = 1,
            max_vacancies: Optional[int] = 1000,
            max_pages: int = 20,
            show_plots: bool = False,
            tech_keywords: Optional[List[str]] = None,
            process_descriptions: bool = True
    ) -> Dict:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.

        Args:
            query: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞
            max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π (None = –≤—Å–µ)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            show_plots: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫–∏
            tech_keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            process_descriptions: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π/–∑–∞–¥–∞—á

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∞–Ω–∞–ª–∏–∑–∞
        """
        safe_query = query.replace(' ', '_').replace('/', '_').lower()
        output_dir = self.output_dir / safe_query
        output_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'=' * 60}")
        print(f"üìä –ê–Ω–∞–ª–∏–∑: {query}")
        print(f"{'=' * 60}")

        # 1. –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π
        vacancies_list = self.searcher.search(
            query,
            area,
            max_pages=max_pages,
            max_vacancies=max_vacancies
        )

        if not vacancies_list:
            print(f"‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è: {query}")
            return {}

        # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        detailed_vacancies = self.details_fetcher.fetch_details(vacancies_list)

        if not detailed_vacancies:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –¥–ª—è: {query}")
            return {}

        # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.json_saver.save(
            detailed_vacancies,
            str(output_dir / 'raw.json')
        )

        # 4. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö...")
        analyzer = self.analyzer_class(detailed_vacancies)
        df = analyzer.extract_data()

        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.csv_saver.save(df, str(output_dir / 'processed.csv'))

        # 6. –ê–Ω–∞–ª–∏–∑ –Ω–∞–≤—ã–∫–æ–≤
        skills_df = analyzer.analyze_skills()
        if len(skills_df) > 0:
            self.csv_saver.save(skills_df, str(output_dir / 'skills.csv'))

        # 7. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π (–∏–∑ key_skills)
        requirements_df = analyzer.analyze_requirements(tech_keywords)
        if len(requirements_df) > 0:
            self.csv_saver.save(
                requirements_df,
                str(output_dir / 'requirements.csv')
            )

        # 8. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º
        salary_stats = analyzer.get_salary_stats()
        self.json_saver.save(
            salary_stats,
            str(output_dir / 'salary_stats.json')
        )

        # 9. –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
        companies_df = analyzer.analyze_by_company(top_n=20)
        if len(companies_df) > 0:
            self.csv_saver.save(companies_df, str(output_dir / 'companies.csv'))

        # 10. –ê–Ω–∞–ª–∏–∑ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É —Ä–∞–±–æ—Ç—ã
        schedule_df = analyzer.analyze_by_schedule()
        if len(schedule_df) > 0:
            self.csv_saver.save(schedule_df, str(output_dir / 'schedule.csv'))

        # 11. –ê–Ω–∞–ª–∏–∑ –ø–æ —Å—Ç–∞–Ω—Ü–∏—è–º –º–µ—Ç—Ä–æ
        metro_df = analyzer.analyze_by_metro(top_n=20)
        if len(metro_df) > 0 and metro_df.iloc[0]['–°—Ç–∞–Ω—Ü–∏—è –º–µ—Ç—Ä–æ'] != '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö':
            self.csv_saver.save(metro_df, str(output_dir / 'metro.csv'))

        # ========== –û–ë–†–ê–ë–û–¢–ö–ê –û–ü–ò–°–ê–ù–ò–ô ==========

        description_processor = None

        if process_descriptions:
            description_processor = self._process_vacancy_descriptions(
                detailed_vacancies,
                output_dir,
                tech_keywords
            )

        # =========================================

        # 12. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self.visualizer.visualize(analyzer, str(output_dir), show_plots)

        # 13. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
        top_skills = (
            skills_df.head(5)['–ù–∞–≤—ã–∫'].tolist()
            if len(skills_df) > 0 else []
        )

        top_companies = (
            companies_df.head(3)['–ö–æ–º–ø–∞–Ω–∏—è'].tolist()
            if len(companies_df) > 0 else []
        )

        summary = {
            '–î–æ–ª–∂–Ω–æ—Å—Ç—å': query,
            '–í–∞–∫–∞–Ω—Å–∏–π —Å–æ–±—Ä–∞–Ω–æ': len(df),
            '–¢–æ–ø-5 –Ω–∞–≤—ã–∫–æ–≤': ', '.join(top_skills),
            '–¢–æ–ø-3 –∫–æ–º–ø–∞–Ω–∏–∏': ', '.join(top_companies),
            '–°—Ä–µ–¥–Ω—è—è –ó–ü (–æ—Ç)': salary_stats.get('avg_from', 'N/A'),
            '–ú–µ–¥–∏–∞–Ω–∞ –ó–ü (–æ—Ç)': salary_stats.get('median_from', 'N/A'),
            '–ü–∞–ø–∫–∞': str(output_dir)
        }

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—è–º
        if description_processor:
            desc_stats = description_processor.get_statistics()
            summary.update({
                '–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π': desc_stats.get('total_requirements_extracted', 0),
                '–ò–∑–≤–ª–µ—á–µ–Ω–æ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π': desc_stats.get('total_responsibilities_extracted', 0),
            })

        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è: {query}")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {output_dir}")

        return summary

    def _process_vacancy_descriptions(
            self,
            detailed_vacancies: List[Dict],
            output_dir: Path,
            tech_keywords: Optional[List[str]] = None
    ) -> IDescriptionProcessor:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π.

        Args:
            detailed_vacancies: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            tech_keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π

        Returns:
            –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π
        """
        print(f"\nüìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π...")

        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        text_cleaner = HtmlTextCleaner(preserve_structure=True)

        # –í—ã–±–æ—Ä —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        if tech_keywords:
            requirements_extractor = SkillsBasedRequirementsExtractor(
                tech_keywords=tech_keywords,
                min_length=Config.REQ_MIN_LENGTH,
                max_length=Config.REQ_MAX_LENGTH,
                min_words=Config.REQ_MIN_WORDS,
                similarity_threshold=Config.SIMILARITY_THRESHOLD
            )
        else:
            requirements_extractor = RequirementsExtractor(
                min_length=Config.REQ_MIN_LENGTH,
                max_length=Config.REQ_MAX_LENGTH,
                min_words=Config.REQ_MIN_WORDS,
                similarity_threshold=Config.SIMILARITY_THRESHOLD
            )

        # ========== –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ Config ==========
        responsibilities_extractor = ResponsibilitiesExtractor(
            min_length=Config.RESP_MIN_LENGTH,
            max_length=Config.RESP_MAX_LENGTH,
            min_words=Config.RESP_MIN_WORDS,
            similarity_threshold=Config.SIMILARITY_THRESHOLD
        )
        # ================================================================

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º
        processor = VacancyDescriptionProcessor(
            text_cleaner=text_cleaner,
            requirements_extractor=requirements_extractor,
            responsibilities_extractor=responsibilities_extractor,
            use_classifier=Config.USE_CLASSIFIER
        )

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π
        df = processor.process_vacancies(detailed_vacancies)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if len(df) > 0:
            self.csv_saver.save(
                df,
                str(output_dir / 'extracted_requirements_responsibilities.csv')
            )
            print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: extracted_requirements_responsibilities.csv")

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        req_freq = processor.get_requirements_frequency()
        if len(req_freq) > 0:
            self.csv_saver.save(
                req_freq,
                str(output_dir / 'requirements_frequency.csv')
            )
            print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: requirements_frequency.csv")
            print(f"\nüìä –¢–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π:")
            for idx, row in req_freq.head(10).iterrows():
                print(f"   {idx + 1}. {row['–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ'][:60]}... ({row['–ß–∞—Å—Ç–æ—Ç–∞']} - {row['–ü—Ä–æ—Ü–µ–Ω—Ç']}%)")

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π
        resp_freq = processor.get_responsibilities_frequency()
        if len(resp_freq) > 0:
            self.csv_saver.save(
                resp_freq,
                str(output_dir / 'responsibilities_frequency.csv')
            )
            print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: responsibilities_frequency.csv")
            print(f"\nüìä –¢–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π:")
            for idx, row in resp_freq.head(10).iterrows():
                print(f"   {idx + 1}. {row['–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å'][:60]}... ({row['–ß–∞—Å—Ç–æ—Ç–∞']} - {row['–ü—Ä–æ—Ü–µ–Ω—Ç']}%)")

        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON
        detailed_data = processor.get_detailed_vacancy_data()
        self.json_saver.save(
            detailed_data,
            str(output_dir / 'detailed_extracted_data.json')
        )
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: detailed_extracted_data.json")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        stats = processor.get_statistics()
        self.json_saver.save(
            stats,
            str(output_dir / 'description_processing_stats.json')
        )
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: description_processing_stats.json")
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        print(f"   –í–∞–∫–∞–Ω—Å–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('total_vacancies_processed', 0)}")
        print(f"   –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {'–î–ê' if stats.get('classifier_used') else '–ù–ï–¢'}")
        print(f"   –¢—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–æ: {stats.get('total_requirements_extracted', 0)}")
        print(f"   –û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–æ: {stats.get('total_responsibilities_extracted', 0)}")

        return processor

    def process_batch_queries(
            self,
            queries: List[str],
            area: int = 1,
            max_vacancies: Optional[int] = 1000,
            max_pages: int = 20,
            show_plots: bool = False,
            tech_keywords: Optional[List[str]] = None,
            process_descriptions: bool = True
    ) -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.

        Args:
            queries: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
            area: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞
            max_vacancies: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π (None = –≤—Å–µ)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            show_plots: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫–∏
            tech_keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            process_descriptions: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π/–∑–∞–¥–∞—á

        Returns:
            DataFrame —Å–æ —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º
        """
        print(f"\n{'=' * 60}")
        print(f"üîÑ Batch-–∞–Ω–∞–ª–∏–∑: {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"{'=' * 60}")

        summaries = []

        for i, query in enumerate(queries, 1):
            print(f"\n[{i}/{len(queries)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {query}")

            summary = self.process_single_query(
                query=query,
                area=area,
                max_vacancies=max_vacancies,
                max_pages=max_pages,
                show_plots=show_plots,
                tech_keywords=tech_keywords,
                process_descriptions=process_descriptions
            )

            if summary:
                summaries.append(summary)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        if summaries:
            summary_df = pd.DataFrame(summaries)
            summary_path = self.output_dir / 'batch_summary.csv'
            self.csv_saver.save(summary_df, str(summary_path))

            print(f"\n{'=' * 60}")
            print(f"‚úÖ Batch-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(summaries)}/{len(queries)}")
            print(f"üìÅ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç: {summary_path}")
            print(f"{'=' * 60}\n")

            return summary_df
        else:
            print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å")
            return pd.DataFrame()
