"""
–ú–æ–¥—É–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏ –∑–∞–¥–∞—á –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏.
–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —à—É–º–∞.
"""

import re
from typing import List
from difflib import SequenceMatcher
from core.interfaces import ITextSectionExtractor


class ResponsibilitiesExtractor(ITextSectionExtractor):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏ –∑–∞–¥–∞—á –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏."""

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–µ–∫—Ü–∏–π —Å –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—è–º–∏
    RESPONSIBILITY_HEADERS = [
        r'–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:',
        r'–≤–∞—à–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:',
        r'–≤ –≤–∞—à–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∏—Ç:',
        r'–∑–∞–¥–∞—á–∏:',
        r'–≤–∞—à–∏ –∑–∞–¥–∞—á–∏:',
        r'–≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:',
        r'—á–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:',
        r'responsibilities:',
        r'duties:',
        r'what you.*?ll do:',
        r'you will:',
        r'your responsibilities:',
        r'–≤ —Ä–∞–±–æ—Ç–µ:',
        r'—á—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å:',
        r'—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:',
        r'–æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:',
        r'—á–µ–º –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:',
        r'—á–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:',
        r'–ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á:',
        r'—á—Ç–æ –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–µ–ª–∞—Ç—å:',
        r'–∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏:',
    ]

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã-–º–∞—Ä–∫–µ—Ä—ã –∑–∞–¥–∞—á
    TASK_MARKERS = [
        r'\b(?:—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞|—Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å|develop)',
        r'\b(?:–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ|–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å|design)',
        r'\b(?:–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ|–≤–Ω–µ–¥—Ä–∏—Ç—å|implement)',
        r'\b(?:–ø–æ–¥–¥–µ—Ä–∂–∫–∞|–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å|maintain)',
        r'\b(?:–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è|–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å|optimize)',
        r'\b(?:—É—á–∞—Å—Ç–∏–µ –≤|participation)',
        r'\b(?:—Ä–∞–±–æ—Ç–∞ —Å|working with)',
        r'\b(?:–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ|collaborate)',
        r'\b(?:—Å–æ–∑–¥–∞–Ω–∏–µ|—Å–æ–∑–¥–∞–≤–∞—Ç—å|create)',
        r'\b(?:–∞–Ω–∞–ª–∏–∑|–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å|analyze)',
        r'\b(?:—Å–±–æ—Ä|—Å–æ–±–∏—Ä–∞—Ç—å|collect)',
        r'\b(?:—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ|—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ|—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–Ω–∞—Å—Ç—Ä–æ–π–∫–∞|–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å)',
        r'\b(?:–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è|–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–æ–±–µ—Å–ø–µ—á–∏—Ç—å|–æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å)',
        r'\b(?:–æ–ø–∏—Å–∞–Ω–∏–µ|–æ–ø–∏—Å—ã–≤–∞—Ç—å)',
        r'\b(?:–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è|–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–∫–æ–Ω—Ç—Ä–æ–ª—å|–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–≤–µ–¥–µ–Ω–∏–µ|–≤–µ—Å—Ç–∏)',
        r'\b(?:–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞|–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å)',
        r'\b(?:–ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ|–ø–æ—Å—Ç—Ä–æ–∏—Ç—å)',
        r'\b(?:–ø–æ–∏—Å–∫|–∏—Å–∫–∞—Ç—å)',
    ]

    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï —Å—Ç–æ–ø-—Å–µ–∫—Ü–∏–∏
    STOP_SECTION_HEADERS = [
        r'(?:—á—Ç–æ )?–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:?',
        r'—É—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã?:?',
        r'—É—Å–ª–æ–≤–∏—è:?',
        r'we offer:?',
        r'benefits:?',
        r'–æ –∫–æ–º–ø–∞–Ω–∏–∏:?',
        r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:?',
        r'requirements:?',
        r'–∑–∞—Ä–ø–ª–∞—Ç–∞:?',
        r'–±–µ–Ω–µ—Ñ–∏—Ç—ã:?',
        r'–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∂–∏–∑–Ω—å:?',
        r'–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã:?',
        r'—Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã:?',
        r'–Ω–∞—à–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:?',
        r'–ø–æ—á–µ–º—É –º—ã:?',
        r'–Ω–∞–º –≤–∞–∂–Ω–æ:?',
        r'–æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:?',
    ]

    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã
    NOISE_PHRASES = [
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–ø–∞–Ω–∏–∏
        r'^–∑–∞\s+\d+\s+(?:–ª–µ—Ç|–≥–æ–¥–∞|–≥–æ–¥)',
        r'^–º—ã –ø–æ–º–æ–≥–ª–∏',
        r'–ø–æ–º–æ–≥–ª–∏\s+\d+\+?\s*–∫–æ–º–ø–∞–Ω–∏',
        r'–≤–Ω–µ–¥—Ä–∏—Ç—å\s+—Å–∏—Å—Ç–µ–º–Ω–æ–µ\s+—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ',
        r'–ø–æ–≤—ã—Å–∏—Ç—å\s+—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å',
        r'—Å–æ–∫—Ä–∞—Ç–∏—Ç—å\s+–∑–∞—Ç—Ä–∞—Ç—ã',

        # –ß—Ç–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—è
        r'^–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º',
        r'^—á—Ç–æ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º',
        r'^—É—Å–ª–æ–≤–∏—è',
        r'^–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ',
        r'^—É–¥–∞–ª[–µ—ë]–Ω–Ω?—ã–π —Ñ–æ—Ä–º–∞—Ç',
        r'^–≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç',
        r'^–≥—Ä–∞—Ñ–∏–∫',
        r'^–∑–∞—Ä–ø–ª–∞—Ç–∞',
        r'^\d+/\d+',
        r'^–æ—Ñ–∏—Å',
        r'^–¥–º—Å',
        r'–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤',
        r'—Ç–∏–º–±–∏–ª–¥–∏–Ω–≥',

        # –û –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥
        r'^–Ω–∞—à–∞ —Ü–µ–ª—å',
        r'^–º—ã (?:—è–≤–ª—è–µ–º—Å—è|–∑–∞–Ω–∏–º–∞–µ–º—Å—è|—Å–æ–∑–¥–∞–µ–º)',
        r'^–∫–æ–º–ø–∞–Ω–∏—è (?:—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è|—Ä–∞–±–æ—Ç–∞–µ—Ç)',
        r'^–µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ',
        r'^–±—É–¥–µ–º —Ä–∞–¥—ã',
        r'^–∂–¥–µ–º (?:–≤–∞—Å|—Ç–µ–±—è)',
        r'^–ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å',

        # –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –±–µ–Ω–µ—Ñ–∏—Ç—ã
        r'^—É–Ω–∏–∫–∞–ª—å–Ω[–∞—ã][—è–π] —ç–∫—Å–ø–µ—Ä—Ç–∏–∑',
        r'^–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç',
        r'^–∫–∞—Ä—å–µ—Ä–Ω—ã–π —Ä–æ—Å—Ç',
        r'^–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å (?:—Ä–∞–∑–≤–∏—Ç–∏—è|—Ä–æ—Å—Ç–∞|–æ–±—É—á–µ–Ω–∏—è)',
        r'^–Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ',
        r'^–æ–±—É—á–µ–Ω–∏–µ (?:–≤–Ω—É—Ç—Ä–∏|–∑–∞ —Å—á–µ—Ç)',
        r'^–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω[–∞—ã][—è–π] –∑–∞—Ä–ø–ª–∞—Ç',
        r'^–∫–æ–º—Ñ–æ—Ä—Ç–Ω[–∞—ã][—è–π]',
        r'^–≥–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫',
        r'^—É–¥–∞–ª–µ–Ω–Ω[–∞—ã][—è–π] —Ä–∞–±–æ—Ç',
        r'^–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ',
        r'^—Ä–∞–±–æ—Ç–∞ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–π—Å—è',

        # –û–±—â–∏–µ —Ñ—Ä–∞–∑—ã
        r'^–æ—Ç–ª–∏—á–Ω',
        r'^–ø—Ä–µ–∫—Ä–∞—Å–Ω',
        r'^–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞',
        r'^–±–æ–Ω—É—Å—ã',
        r'^–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è',
        r'^—Å—Ç–∞–±–∏–ª—å–Ω–∞—è',
        r'^–∞–¥–µ–∫–≤–∞—Ç–Ω–æ–µ',

        # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è, –ø–æ–ø–∞–≤—à–∏–µ –≤ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏
        r'^–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã',
        r'^–∑–Ω–∞–Ω–∏–µ',
        r'^–≤–ª–∞–¥–µ–Ω–∏–µ',
        r'^—É–º–µ–Ω–∏–µ.*?(?:—Ä–∞–±–æ—Ç–∞—Ç—å —Å|–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)',
        r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ.*?(?:–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤|–æ—Å–Ω–æ–≤)',
        r'^–Ω–∞–≤—ã–∫[–∏–∏]?\s+(?:—Ä–∞–±–æ—Ç—ã —Å|–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)',
        r'^–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
        r'^–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
        r'^—É–≤–µ—Ä–µ–Ω–Ω–æ–µ',
        r'^–≥–ª—É–±–æ–∫–æ–µ',
        r'^—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –Ω–∞—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—å',
        r'^–±–∞–∑–æ–≤—ã–µ –æ—Å–Ω–æ–≤—ã',
        r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Å–Ω–æ–≤',

        # –≠–º–æ–¥–∑–∏
        r'üì©|üìß|‚úâÔ∏è|üíº|üéØ|üöÄ|‚≠ê|‚ú®|üîç|üìä|üìà|üí∞|üèÜ',
    ]

    def __init__(
            self,
            min_length: int = 20,
            max_length: int = 450,
            min_words: int = 4,
            similarity_threshold: float = 0.85
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π."""
        self.min_length = min_length
        self.max_length = max_length
        self.min_words = min_words
        self.similarity_threshold = similarity_threshold
        self._compile_patterns()

    def _compile_patterns(self):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π."""
        self.header_pattern = re.compile(
            r'(?:^|\n)\s*(?:' + '|'.join(self.RESPONSIBILITY_HEADERS) + r')\s*(?:\n|$)',
            re.IGNORECASE | re.MULTILINE | re.UNICODE
        )
        self.marker_pattern = re.compile(
            r'(?:' + '|'.join(self.TASK_MARKERS) + r')',
            re.IGNORECASE | re.UNICODE
        )
        self.stop_section_pattern = re.compile(
            r'(?:^|\n)\s*(?:' + '|'.join(self.STOP_SECTION_HEADERS) + r')',
            re.IGNORECASE | re.MULTILINE | re.UNICODE
        )
        self.noise_pattern = re.compile(
            '|'.join(self.NOISE_PHRASES),
            re.IGNORECASE | re.UNICODE
        )

    def extract(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return []

        responsibilities = []

        section_resp = self._extract_from_sections(text)
        responsibilities.extend(section_resp)

        marker_resp = self._extract_by_markers(text)
        responsibilities.extend(marker_resp)

        list_resp = self._extract_from_lists(text)
        responsibilities.extend(list_resp)

        responsibilities = self._advanced_clean_and_deduplicate(responsibilities)

        return responsibilities

    def _extract_from_sections(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–µ–∫—Ü–∏–π."""
        responsibilities = []

        for match in self.header_pattern.finditer(text):
            section_start = match.end()
            section_end = self._find_section_end(text, section_start)

            if section_end is None:
                section_end = len(text)

            section_text = text[section_start:section_end]

            if not self.stop_section_pattern.search(match.group()):
                items = self._split_into_items(section_text)
                responsibilities.extend(items)

        return responsibilities

    def _find_section_end(self, text: str, start_pos: int) -> int:
        """–ü–æ–∏—Å–∫ –∫–æ–Ω—Ü–∞ —Å–µ–∫—Ü–∏–∏."""
        next_headers_pattern = re.compile(
            r'\n\s*(?:'
            r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è|'
            r'—É—Å–ª–æ–≤–∏—è|'
            r'–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º|'
            r'what we offer|'
            r'requirements|'
            r'–æ –∫–æ–º–ø–∞–Ω–∏–∏|'
            r'–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|'
            r'–∑–∞–¥–∞—á–∏|'
            r'–Ω–∞–º –≤–∞–∂–Ω–æ|'
            r'–æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞'
            r'):',
            re.IGNORECASE | re.UNICODE
        )

        match = next_headers_pattern.search(text[start_pos:])

        if match:
            return start_pos + match.start()

        return len(text)

    def _extract_by_markers(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º."""
        responsibilities = []
        sentences = re.split(r'[.;!?]\s+', text)

        for sentence in sentences:
            if self.marker_pattern.search(sentence):
                cleaned = sentence.strip()

                if self._is_valid_responsibility(cleaned):
                    responsibilities.append(cleaned)

        return responsibilities

    def _extract_from_lists(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–æ–≤."""
        responsibilities = []

        list_patterns = [
            r'^[-‚Ä¢*]\s*(.+)$',
            r'^\d+[\.)]\s*(.+)$',
        ]

        lines = text.split('\n')

        for line in lines:
            for pattern in list_patterns:
                match = re.match(pattern, line.strip())

                if match:
                    item = match.group(1).strip()

                    if self._is_valid_responsibility(item):
                        responsibilities.append(item)
                    break

        return responsibilities

    def _split_into_items(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã."""
        items = []

        separators = [';', '\n']
        current_items = [text]

        for sep in separators:
            new_items = []
            for item in current_items:
                new_items.extend(item.split(sep))
            current_items = new_items

        final_items = []
        for item in current_items:
            parts = re.split(r'\.\s+(?=[–ê-–ØA-Z–Å])', item)
            final_items.extend(parts)

        for item in final_items:
            cleaned = self._clean_item(item)

            if self._is_valid_responsibility(cleaned):
                items.append(cleaned)

        return items

    def _clean_item(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        # –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏
        text = re.sub(r'[\U0001F300-\U0001F9FF]', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ —Å–ø–∏—Å–∫–æ–≤
        text = re.sub(r'^[-‚Ä¢*\d+\.)]\s*', '', text)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())

        # –£–¥–∞–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ
        text = text.rstrip(';').rstrip('.')

        return text.strip()

    def _is_valid_responsibility(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏."""
        if not text:
            return False

        if len(text) < self.min_length or len(text) > self.max_length:
            return False

        words = text.split()
        if len(words) < self.min_words:
            return False

        if text.strip().endswith(':'):
            return False

        if self.noise_pattern.search(text):
            return False

        if re.match(r'^\d+[\s\-/]*\d*$', text.strip()):
            return False

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—Å–∫–ª—é—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        requirement_patterns = [
            r'^–æ–ø—ã—Ç\s+—Ä–∞–±–æ—Ç—ã',
            r'^–æ–ø—ã—Ç\s+–æ—Ç',
            r'^–∑–Ω–∞–Ω–∏–µ\s+',
            r'^–≤–ª–∞–¥–µ–Ω–∏–µ\s+',
            r'^—É–º–µ–Ω–∏–µ\s+—Ä–∞–±–æ—Ç–∞—Ç—å\s+—Å\s+',
            r'^–Ω–∞–≤—ã–∫[–∏–∏]?\s+',
            r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ\s+–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤',
            r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ\s+–æ—Å–Ω–æ–≤',
            r'^–≤—ã—Å—à–µ–µ\s+–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
            r'^–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
            r'^—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è\s+–Ω–∞—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—å',
            r'^–±–∞–∑–æ–≤—ã–µ\s+–æ—Å–Ω–æ–≤—ã',
            r'^—É–≤–µ—Ä–µ–Ω–Ω[–æ—ã][–µ–π]\s+(?:–≤–ª–∞–¥–µ–Ω–∏–µ|–∑–Ω–∞–Ω–∏–µ)',
        ]

        text_lower = text.lower()
        for pattern in requirement_patterns:
            if re.search(pattern, text_lower):
                return False

        return True

    def _advanced_clean_and_deduplicate(self, responsibilities: List[str]) -> List[str]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è."""
        if not responsibilities:
            return []

        cleaned = [r for r in responsibilities if self._is_valid_responsibility(r)]

        unique_resp = []
        seen_normalized = set()

        for resp in cleaned:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            normalized = self._normalize_for_comparison(resp)

            if normalized in seen_normalized:
                continue

            is_duplicate = False

            for existing in unique_resp:
                similarity = self._calculate_similarity(resp, existing)

                if similarity >= self.similarity_threshold:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_resp.append(resp)
                seen_normalized.add(normalized)

        return unique_resp

    def _normalize_for_comparison(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()

        # –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
        text = text.rstrip('.;,')

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())

        return text

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏."""
        norm1 = self._normalize_for_comparison(text1)
        norm2 = self._normalize_for_comparison(text2)

        return SequenceMatcher(None, norm1, norm2).ratio()
