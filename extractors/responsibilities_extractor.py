"""
–ú–æ–¥—É–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏ –∑–∞–¥–∞—á –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏.
–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø v2 —Å —É—á–µ—Ç–æ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
"""

import re
from typing import List
from difflib import SequenceMatcher
from core.interfaces import ITextSectionExtractor


class ResponsibilitiesExtractor(ITextSectionExtractor):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏ –∑–∞–¥–∞—á –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏."""

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–µ–∫—Ü–∏–π —Å –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—è–º–∏
    RESPONSIBILITY_HEADERS = [
        r'–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:',
        r'–≤–∞—à–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:',
        r'–≤ –≤–∞—à–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∏—Ç:',
        r'–∑–∞–¥–∞—á–∏:',
        r'–≤–∞—à–∏ –∑–∞–¥–∞—á–∏:',
        r'–≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:?',
        r'—á–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:?',
        r'responsibilities:',
        r'duties:',
        r'what you.*?ll do:',
        r'you will:',
        r'your responsibilities:',
        r'–≤ —Ä–∞–±–æ—Ç–µ:',
        r'—á—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å:',
        r'—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:',
        r'–æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:',
        r'—á–µ–º –∑–∞–Ω–∏–º–∞—Ç—å—Å—è:',
        r'—á–µ–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:',
        r'–ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á:',
        r'—á—Ç–æ –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–µ–ª–∞—Ç—å:',
        r'–∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏:?',
        r'–ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ—à–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:',
        r'–≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ—à–∞—Ç—å:',
    ]

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã-–º–∞—Ä–∫–µ—Ä—ã –∑–∞–¥–∞—á (–∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤—ã –∏ –æ—Ç–≥–ª–∞–≥–æ–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ)
    TASK_MARKERS = [
        r'\b(?:—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞|—Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å|develop)',
        r'\b(?:–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ|–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å|design)',
        r'\b(?:–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ|–≤–Ω–µ–¥—Ä–∏—Ç—å|implement)',
        r'\b(?:–ø–æ–¥–¥–µ—Ä–∂–∫–∞|–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å|maintain)',
        r'\b(?:–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è|–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å|optimize)',
        r'\b(?:—É—á–∞—Å—Ç–∏–µ –≤|participation)',
        r'\b(?:—Ä–∞–±–æ—Ç–∞ —Å|working with)',
        r'\b(?:–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ|collaborate)',
        r'\b(?:—Å–æ–∑–¥–∞–Ω–∏–µ|—Å–æ–∑–¥–∞–≤–∞—Ç—å|create)',
        r'\b(?:–∞–Ω–∞–ª–∏–∑|–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å|analyze)',
        r'\b(?:—Å–±–æ—Ä|—Å–æ–±–∏—Ä–∞—Ç—å|collect)',
        r'\b(?:—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ|—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ|—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–Ω–∞—Å—Ç—Ä–æ–π–∫–∞|–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å)',
        r'\b(?:–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è|–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–æ–±–µ—Å–ø–µ—á–∏—Ç—å|–æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å)',
        r'\b(?:–æ–ø–∏—Å–∞–Ω–∏–µ|–æ–ø–∏—Å—ã–≤–∞—Ç—å)',
        r'\b(?:–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è|–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–∫–æ–Ω—Ç—Ä–æ–ª—å|–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–≤–µ–¥–µ–Ω–∏–µ|–≤–µ—Å—Ç–∏)',
        r'\b(?:–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞|–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å)',
        r'\b(?:–ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ|–ø–æ—Å—Ç—Ä–æ–∏—Ç—å)',
        r'\b(?:–ø–æ–∏—Å–∫|–∏—Å–∫–∞—Ç—å)',
        r'\b(?:–ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ|–ø—Ä–æ–≤–æ–¥–∏—Ç—å)',
        r'\b(?:–≤—ã—è–≤–ª–µ–Ω–∏–µ|–≤—ã—è–≤–ª—è—Ç—å)',
        r'\b(?:–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ|–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ|–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å)',
        r'\b(?:—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è|—Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å)',
        r'\b(?:—Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ|—Å–æ—Å—Ç–∞–≤–ª—è—Ç—å)',
        r'\b(?:–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ|–æ–±—Å–ª–µ–¥–æ–≤–∞—Ç—å)',
    ]

    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï —Å—Ç–æ–ø-—Å–µ–∫—Ü–∏–∏
    STOP_SECTION_HEADERS = [
        r'(?:—á—Ç–æ )?–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:?',
        r'—É—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã?:?',
        r'—É—Å–ª–æ–≤–∏—è:?',
        r'we offer:?',
        r'benefits:?',
        r'–æ –∫–æ–º–ø–∞–Ω–∏–∏:?',
        r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:?',
        r'requirements:?',
        r'–∑–∞—Ä–ø–ª–∞—Ç–∞:?',
        r'–±–µ–Ω–µ—Ñ–∏—Ç—ã:?',
        r'–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∂–∏–∑–Ω—å:?',
        r'–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã:?',
        r'—Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã:?',
        r'–Ω–∞—à–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:?',
        r'–ø–æ—á–µ–º—É –º—ã:?',
        r'–Ω–∞–º –≤–∞–∂–Ω–æ:?',
        r'–æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:?',
        r'–º—ã –æ–∂–∏–¥–∞–µ–º:?',
        r'—á—Ç–æ –∂–¥–µ–º:?',
    ]

    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã –¥–ª—è –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π (—ç—Ç–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è!)
    NOISE_PHRASES = [
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–ø–∞–Ω–∏–∏
        r'^–∑–∞\s+\d+\s+(?:–ª–µ—Ç|–≥–æ–¥–∞|–≥–æ–¥)',
        r'^–º—ã –ø–æ–º–æ–≥–ª–∏',
        r'–ø–æ–º–æ–≥–ª–∏\s+\d+\+?\s*–∫–æ–º–ø–∞–Ω–∏',
        r'–≤–Ω–µ–¥—Ä–∏—Ç—å\s+—Å–∏—Å—Ç–µ–º–Ω–æ–µ\s+—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ',
        r'–ø–æ–≤—ã—Å–∏—Ç—å\s+—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å',
        r'—Å–æ–∫—Ä–∞—Ç–∏—Ç—å\s+–∑–∞—Ç—Ä–∞—Ç—ã',

        # –ß—Ç–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—è
        r'^–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º',
        r'^—á—Ç–æ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º',
        r'^—É—Å–ª–æ–≤–∏—è',
        r'^–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ',
        r'^—É–¥–∞–ª[–µ—ë]–Ω–Ω?—ã–π —Ñ–æ—Ä–º–∞—Ç',
        r'^–≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç',
        r'^–≥—Ä–∞—Ñ–∏–∫',
        r'^–∑–∞—Ä–ø–ª–∞—Ç–∞',
        r'^\d+/\d+',
        r'^–æ—Ñ–∏—Å',
        r'^–¥–º—Å',
        r'–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤',
        r'—Ç–∏–º–±–∏–ª–¥–∏–Ω–≥',

        # –û –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥
        r'^–Ω–∞—à–∞ —Ü–µ–ª—å',
        r'^–º—ã (?:—è–≤–ª—è–µ–º—Å—è|–∑–∞–Ω–∏–º–∞–µ–º—Å—è|—Å–æ–∑–¥–∞–µ–º)',
        r'^–∫–æ–º–ø–∞–Ω–∏—è (?:—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è|—Ä–∞–±–æ—Ç–∞–µ—Ç)',
        r'^–µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ',
        r'^–±—É–¥–µ–º —Ä–∞–¥—ã',
        r'^–∂–¥–µ–º (?:–≤–∞—Å|—Ç–µ–±—è)',
        r'^–ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å',

        # –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –±–µ–Ω–µ—Ñ–∏—Ç—ã
        r'^—É–Ω–∏–∫–∞–ª—å–Ω[–∞—ã][—è–π] —ç–∫—Å–ø–µ—Ä—Ç–∏–∑',
        r'^–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç',
        r'^–∫–∞—Ä—å–µ—Ä–Ω—ã–π —Ä–æ—Å—Ç',
        r'^–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å (?:—Ä–∞–∑–≤–∏—Ç–∏—è|—Ä–æ—Å—Ç–∞|–æ–±—É—á–µ–Ω–∏—è)',
        r'^–Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ',
        r'^–æ–±—É—á–µ–Ω–∏–µ (?:–≤–Ω—É—Ç—Ä–∏|–∑–∞ —Å—á–µ—Ç)',
        r'^–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω[–∞—ã][—è–π] –∑–∞—Ä–ø–ª–∞—Ç',
        r'^–∫–æ–º—Ñ–æ—Ä—Ç–Ω[–∞—ã][—è–π]',
        r'^–≥–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫',
        r'^—É–¥–∞–ª–µ–Ω–Ω[–∞—ã][—è–π] —Ä–∞–±–æ—Ç',
        r'^–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ',
        r'^—Ä–∞–±–æ—Ç–∞ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–π—Å—è',

        # –û–±—â–∏–µ —Ñ—Ä–∞–∑—ã
        r'^–æ—Ç–ª–∏—á–Ω',
        r'^–ø—Ä–µ–∫—Ä–∞—Å–Ω',
        r'^–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞',
        r'^–±–æ–Ω—É—Å—ã',
        r'^–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è',
        r'^—Å—Ç–∞–±–∏–ª—å–Ω–∞—è',
        r'^–∞–¥–µ–∫–≤–∞—Ç–Ω–æ–µ',

        # –í–ê–ñ–ù–û: –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è, –ø–æ–ø–∞–≤—à–∏–µ –≤ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏
        r'^–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã',
        r'^–æ–ø—ã—Ç –æ—Ç',
        r'^–æ–ø—ã—Ç.*?(?:–æ—Ç|–Ω–µ –º–µ–Ω–µ–µ|–±–æ–ª–µ–µ)\s+\d+',
        r'^–∑–Ω–∞–Ω–∏–µ\s+(?!–ø—Ä–æ—Ü–µ—Å—Å–æ–≤|–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π|–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)',
        # "–ó–Ω–∞–Ω–∏–µ SQL" - —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ, –Ω–æ "–ó–Ω–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤" –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–¥–∞—á–∏
        r'^–≤–ª–∞–¥–µ–Ω–∏–µ\s+',
        r'^—É–º–µ–Ω–∏–µ\s+(?:—Ä–∞–±–æ—Ç–∞—Ç—å —Å|–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å|—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å)',
        r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ\s+(?:–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤|–æ—Å–Ω–æ–≤|–∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç)',
        r'^–Ω–∞–≤—ã–∫[–∏–∏]?\s+(?:—Ä–∞–±–æ—Ç—ã —Å|–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è|–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏)',
        r'^–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
        r'^–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\s+(?:–≤ –æ–±–ª–∞—Å—Ç–∏|–∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–µ)',
        r'^—É–≤–µ—Ä–µ–Ω–Ω–æ–µ\s+(?:–∑–Ω–∞–Ω–∏–µ|–≤–ª–∞–¥–µ–Ω–∏–µ)',
        r'^–≥–ª—É–±–æ–∫–æ–µ\s+(?:–∑–Ω–∞–Ω–∏–µ|–ø–æ–Ω–∏–º–∞–Ω–∏–µ)',
        r'^—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –Ω–∞—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—å',
        r'^–±–∞–∑–æ–≤—ã–µ –æ—Å–Ω–æ–≤—ã',
        r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Å–Ω–æ–≤',
        r'^—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç[—å–∏]\s+',
        r'^—Ä–∞–∑–≤–∏—Ç—ã–µ\s+(?:–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ|–∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ)',
        r'^–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫[–∏–æ–∞][–µ–π–º]\s+(?:—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç|—Å–∫–ª–∞–¥ —É–º–∞)',
        r'^–∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω',
        r'^—Å–∏—Å—Ç–µ–º–Ω[–æ–∞][–µ–π–º]\s+–º—ã—à–ª–µ–Ω–∏–µ',
        r'^–≥—Ä–∞–º–æ—Ç–Ω[–∞—ã][—è–π]\s+(?:—É—Å—Ç–Ω–∞—è|–ø–∏—Å—å–º–µ–Ω–Ω–∞—è)',
        r'^–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
        r'^–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å',
        r'^–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å',

        # –§—Ä–∞–∑—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        r'^—á—Ç–æ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è',
        r'^–º—ã –æ–∂–∏–¥–∞–µ–º',
        r'^—á—Ç–æ –∂–¥–µ–º –æ—Ç',
        r'^–Ω–∞–º –≤–∞–∂–Ω–æ',

        # –≠–º–æ–¥–∑–∏
        r'üì©|üìß|‚úâÔ∏è|üíº|üéØ|üöÄ|‚≠ê|‚ú®|üîç|üìä|üìà|üí∞|üèÜ',
    ]

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö-–Ω–∞–≤—ã–∫–æ–≤ (—ç—Ç–æ –ù–ï –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏, –∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
    REQUIREMENT_NOUN_PATTERNS = [
        r'^–æ–ø—ã—Ç\s+',
        r'^–∑–Ω–∞–Ω–∏–µ\s+(?:sql|python|java|excel|—è–∑—ã–∫–∞)',
        r'^–≤–ª–∞–¥–µ–Ω–∏–µ\s+',
        r'^—É–º–µ–Ω–∏–µ\s+(?!—á—ë—Ç–∫–æ –ø–æ–Ω–∏–º–∞—Ç—å –∑–∞–¥–∞—á—É)',  # "–£–º–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—Ç—å" - —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
        r'^–Ω–∞–≤—ã–∫[–∏–∏]?\s+',
        r'^–ø–æ–Ω–∏–º–∞–Ω–∏–µ\s+(?:–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤|–æ—Å–Ω–æ–≤)',
        r'^–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
        r'^—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç[—å–∏]',
    ]

    def __init__(
            self,
            min_length: int = 20,
            max_length: int = 450,
            min_words: int = 4,
            similarity_threshold: float = 0.85
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π."""
        self.min_length = min_length
        self.max_length = max_length
        self.min_words = min_words
        self.similarity_threshold = similarity_threshold
        self._compile_patterns()

    def _compile_patterns(self):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π."""
        self.header_pattern = re.compile(
            r'(?:^|\n)\s*(?:' + '|'.join(self.RESPONSIBILITY_HEADERS) + r')\s*(?:\n|$)',
            re.IGNORECASE | re.MULTILINE | re.UNICODE
        )
        self.marker_pattern = re.compile(
            r'(?:' + '|'.join(self.TASK_MARKERS) + r')',
            re.IGNORECASE | re.UNICODE
        )
        self.stop_section_pattern = re.compile(
            r'(?:^|\n)\s*(?:' + '|'.join(self.STOP_SECTION_HEADERS) + r')',
            re.IGNORECASE | re.MULTILINE | re.UNICODE
        )
        self.noise_pattern = re.compile(
            '|'.join(self.NOISE_PHRASES),
            re.IGNORECASE | re.UNICODE
        )
        self.requirement_noun_pattern = re.compile(
            '|'.join(self.REQUIREMENT_NOUN_PATTERNS),
            re.IGNORECASE | re.UNICODE
        )

    def extract(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return []

        responsibilities = []

        # –ú–µ—Ç–æ–¥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–µ–∫—Ü–∏–π (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π)
        section_resp = self._extract_from_sections(text)
        responsibilities.extend(section_resp)

        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ —Å–µ–∫—Ü–∏–π)
        if len(section_resp) < 3:
            marker_resp = self._extract_by_markers(text)
            responsibilities.extend(marker_resp)

        # –ú–µ—Ç–æ–¥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–æ–≤ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
        if len(responsibilities) < 5:
            list_resp = self._extract_from_lists(text)
            responsibilities.extend(list_resp)

        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        responsibilities = self._advanced_clean_and_deduplicate(responsibilities)

        return responsibilities

    def _extract_from_sections(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–µ–∫—Ü–∏–π."""
        responsibilities = []

        for match in self.header_pattern.finditer(text):
            section_start = match.end()
            section_end = self._find_section_end(text, section_start)

            if section_end is None:
                section_end = len(text)

            section_text = text[section_start:section_end]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å—Ç–æ–ø-—Å–µ–∫—Ü–∏—è
            if not self.stop_section_pattern.search(match.group()):
                items = self._split_into_items(section_text)
                responsibilities.extend(items)

        return responsibilities

    def _find_section_end(self, text: str, start_pos: int) -> int:
        """–ü–æ–∏—Å–∫ –∫–æ–Ω—Ü–∞ —Å–µ–∫—Ü–∏–∏."""
        next_headers_pattern = re.compile(
            r'\n\s*(?:'
            r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è|'
            r'—É—Å–ª–æ–≤–∏—è|'
            r'–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º|'
            r'what we offer|'
            r'requirements|'
            r'–æ –∫–æ–º–ø–∞–Ω–∏–∏|'
            r'–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|'
            r'–∑–∞–¥–∞—á–∏|'
            r'–Ω–∞–º –≤–∞–∂–Ω–æ|'
            r'–æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞|'
            r'–º—ã –æ–∂–∏–¥–∞–µ–º|'
            r'—á—Ç–æ –∂–¥–µ–º'
            r'):',
            re.IGNORECASE | re.UNICODE
        )

        match = next_headers_pattern.search(text[start_pos:])

        if match:
            return start_pos + match.start()

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–µ–∫—Ü–∏—é 1500 —Å–∏–º–≤–æ–ª–∞–º–∏
        return min(start_pos + 1500, len(text))

    def _extract_by_markers(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º."""
        responsibilities = []
        sentences = re.split(r'[.;!?]\s+', text)

        for sentence in sentences:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if len(sentence) > 350:
                continue

            if self.marker_pattern.search(sentence):
                cleaned = sentence.strip()

                if self._is_valid_responsibility(cleaned):
                    responsibilities.append(cleaned)

        return responsibilities

    def _extract_from_lists(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–æ–≤."""
        responsibilities = []

        list_patterns = [
            r'^[-‚Ä¢*]\s*(.+)$',
            r'^\d+[\.)]\s*(.+)$',
        ]

        lines = text.split('\n')

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–≤ –∫–∞–∫–æ–π —Å–µ–∫—Ü–∏–∏ –Ω–∞—Ö–æ–¥–∏–º—Å—è)
        in_responsibilities_section = False

        for line in lines:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–ø–∞–ª–∏ –ª–∏ –º—ã –≤ —Å–µ–∫—Ü–∏—é –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π
            if re.search(r'(?:–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|–∑–∞–¥–∞—á–∏|–≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç|—á–µ–º –∑–∞–Ω–∏–º–∞—Ç—å—Å—è):', line.lower()):
                in_responsibilities_section = True
            elif re.search(r'(?:—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è|–º—ã –æ–∂–∏–¥–∞–µ–º|–Ω–∞–º –≤–∞–∂–Ω–æ|—á—Ç–æ –∂–¥–µ–º):', line.lower()):
                in_responsibilities_section = False

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–µ–∫—Ü–∏–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π
            if in_responsibilities_section:
                for pattern in list_patterns:
                    match = re.match(pattern, line.strip())

                    if match:
                        item = match.group(1).strip()

                        if self._is_valid_responsibility(item):
                            responsibilities.append(item)
                        break

        return responsibilities

    def _split_into_items(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã."""
        items = []

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —è–≤–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
        separators = [';', '\n']
        current_items = [text]

        for sep in separators:
            new_items = []
            for item in current_items:
                parts = item.split(sep)
                new_items.extend(parts)
            current_items = new_items

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏ –∑–∞–≥–ª–∞–≤–Ω–∞—è –±—É–∫–≤–∞)
        final_items = []
        for item in current_items:
            parts = re.split(r'\.\s+(?=[–ê-–ØA-Z–Å])', item)
            final_items.extend(parts)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
        for item in final_items:
            cleaned = self._clean_item(item)

            if self._is_valid_responsibility(cleaned):
                items.append(cleaned)

        return items

    def _clean_item(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        # –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏
        text = re.sub(r'[\U0001F300-\U0001F9FF]', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ —Å–ø–∏—Å–∫–æ–≤
        text = re.sub(r'^[-‚Ä¢*\d+\.)]\s*', '', text)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())

        # –£–¥–∞–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π –∏ —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ
        text = text.rstrip(';').rstrip('.')

        return text.strip()

    def _is_valid_responsibility(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏."""
        if not text:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(text) < self.min_length or len(text) > self.max_length:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤
        words = text.split()
        if len(words) < self.min_words:
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        if text.strip().endswith(':'):
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑
        if self.noise_pattern.search(text):
            return False

        # –í–ê–ñ–ù–û: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ-–Ω–∞–≤—ã–∫–∏)
        if self.requirement_noun_pattern.search(text):
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —á–∏—Å—Ç–æ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if re.match(r'^\d+[\s\-/]*\d*$', text.strip()):
            return False

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—Å–∫–ª—é—á–∞–µ–º —Ñ—Ä–∞–∑—ã, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å –º–∞—Ä–∫–µ—Ä–æ–≤ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        requirement_starts = [
            '–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã', '–æ–ø—ã—Ç –æ—Ç',
            '–∑–Ω–∞–Ω–∏–µ sql', '–∑–Ω–∞–Ω–∏–µ python', '–∑–Ω–∞–Ω–∏–µ java', '–∑–Ω–∞–Ω–∏–µ excel',
            '–∑–Ω–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ', '–∑–Ω–∞–Ω–∏–µ —è–∑—ã–∫–∞',
            '–≤–ª–∞–¥–µ–Ω–∏–µ',
            '–Ω–∞–≤—ã–∫–∏ —Ä–∞–±–æ—Ç—ã —Å', '–Ω–∞–≤—ã–∫–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏',
            '–ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Å–Ω–æ–≤',
            '–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –Ω–∞—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—å',
            '–±–∞–∑–æ–≤—ã–µ –æ—Å–Ω–æ–≤—ã',
            '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å',
            '–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫', '—Å–∏—Å—Ç–µ–º–Ω',
            '–∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω',
            '–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å',
        ]

        text_lower = text.lower()
        for start in requirement_starts:
            if text_lower.startswith(start):
                return False

        return True

    def _advanced_clean_and_deduplicate(self, responsibilities: List[str]) -> List[str]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è."""
        if not responsibilities:
            return []

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π
        cleaned = [r for r in responsibilities if self._is_valid_responsibility(r)]

        unique_resp = []
        seen_normalized = set()

        for resp in cleaned:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            normalized = self._normalize_for_comparison(resp)

            if normalized in seen_normalized:
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏
            is_duplicate = False

            for existing in unique_resp:
                similarity = self._calculate_similarity(resp, existing)

                if similarity >= self.similarity_threshold:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                    if len(resp) > len(existing):
                        unique_resp.remove(existing)
                        seen_normalized.discard(self._normalize_for_comparison(existing))
                        break
                    else:
                        is_duplicate = True
                        break

            if not is_duplicate:
                unique_resp.append(resp)
                seen_normalized.add(normalized)

        return unique_resp

    def _normalize_for_comparison(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()

        # –£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
        text = text.rstrip('.;,')

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())

        return text

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏."""
        norm1 = self._normalize_for_comparison(text1)
        norm2 = self._normalize_for_comparison(text2)

        return SequenceMatcher(None, norm1, norm2).ratio()
