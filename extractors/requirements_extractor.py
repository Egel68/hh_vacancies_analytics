"""
–ú–æ–¥—É–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è.
–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —à—É–º–∞.
"""

import re
from typing import List
from difflib import SequenceMatcher
from core.interfaces import ITextSectionExtractor


class RequirementsExtractor(ITextSectionExtractor):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏."""

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–µ–∫—Ü–∏–π —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏
    REQUIREMENT_HEADERS = [
        r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:?',
        r'—Ç—Ä–µ–±—É–µ–º:?',
        r'–º—ã –æ–∂–∏–¥–∞–µ–º:?',
        r'–æ–∂–∏–¥–∞–µ–º –æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:?',
        r'–∫–∞–Ω–¥–∏–¥–∞—Ç –¥–æ–ª–∂–µ–Ω:?',
        r'–Ω–µ–æ–±—Ö–æ–¥–∏–º[–æ—ã–∞]:?',
        r'–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –Ω–∞–≤—ã–∫–∏:?',
        r'—á—Ç–æ –º—ã –æ–∂–∏–¥–∞–µ–º:?',
        r'—á—Ç–æ –Ω—É–∂–Ω–æ:?',
        r'requirements:?',
        r'must have:?',
        r'qualifications:?',
        r'–Ω–∞–º –≤–∞–∂–Ω–æ:?',
        r'–º—ã –∏—â–µ–º:?',
        r'–∏–¥–µ–∞–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç:?',
        r'–Ω–∞—à –∏–¥–µ–∞–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç:?',
        r'–≤—ã –Ω–∞–º –ø–æ–¥—Ö–æ–¥–∏—Ç–µ:?',
        r'—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:?',
        r'hard skills:?',
        r'–∫–æ–≥–æ –∏—â–µ–º:?',
        r'–Ω–∞—à–∏ –æ–∂–∏–¥–∞–Ω–∏—è:?',
        r'–æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:?',
        r'–¥–ª—è –Ω–∞—Å –≤–∞–∂–Ω–æ:?',
        r'—á—Ç–æ –∂–¥–µ–º:?',
    ]

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã-–º–∞—Ä–∫–µ—Ä—ã —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    REQUIREMENT_MARKERS = [
        r'–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã',
        r'–æ–ø—ã—Ç.*?(?:–æ—Ç|–Ω–µ –º–µ–Ω–µ–µ|–±–æ–ª–µ–µ|\d+)',
        r'–∑–Ω–∞–Ω–∏–µ',
        r'–∑–Ω–∞–Ω–∏—è',
        r'–≤–ª–∞–¥–µ–Ω–∏–µ',
        r'—É–º–µ–Ω–∏–µ',
        r'–Ω–∞–≤—ã–∫[–∏–∏]',
        r'–ø–æ–Ω–∏–º–∞–Ω–∏–µ',
        r'experience',
        r'knowledge',
        r'understanding',
        r'proficiency',
        r'familiar with',
        r'expertise',
        r'—É–º–µ–µ—Ç',
        r'–∑–Ω–∞–µ—Ç',
        r'–∏–º–µ–µ—Ç –æ–ø—ã—Ç',
        r'–ø–æ–Ω–∏–º–∞–µ—Ç',
        r'–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ',
        r'–≤—ã—Å—à–µ–µ',
        r'—É–≤–µ—Ä–µ–Ω–Ω–æ–µ',
        r'–≥–ª—É–±–æ–∫–æ–µ',
    ]

    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï —Å—Ç–æ–ø-—Å–µ–∫—Ü–∏–∏
    STOP_SECTION_HEADERS = [
        r'(?:—á—Ç–æ )?–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:?',
        r'—É—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã?:?',
        r'—É—Å–ª–æ–≤–∏—è:?',
        r'we offer:?',
        r'benefits:?',
        r'–æ –∫–æ–º–ø–∞–Ω–∏–∏:?',
        r'–æ –Ω–∞—Å:?',
        r'about (?:us|company):?',
        r'–∑–∞—Ä–ø–ª–∞—Ç–∞:?',
        r'salary:?',
        r'compensation:?',
        r'–±–µ–Ω–µ—Ñ–∏—Ç—ã:?',
        r'–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∂–∏–∑–Ω—å:?',
        r'–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ:?',
        r'–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã:?',
        r'–ª–æ–∫–∞—Ü–∏—è:?',
        r'location:?',
        r'–æ—Ñ–∏—Å:?',
        r'—Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã:?',
        r'—Å–æ—Ü\.?\s*–ø–∞–∫–µ—Ç:?',
        r'—Ä–∞–∑–≤–∏—Ç–∏–µ:?',
        r'–æ–±—É—á–µ–Ω–∏–µ:?',
        r'–Ω–∞—à–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:?',
        r'–ø–æ—á–µ–º—É –º—ã:?',
        r'–ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å:?',
    ]

    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã
    NOISE_PHRASES = [
        # –ß—Ç–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—è
        r'^–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º',
        r'^—á—Ç–æ –º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º',
        r'^—É—Å–ª–æ–≤–∏—è',
        r'^–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ',
        r'^—Ä–∞–±–æ—Ç–∞ –≤ –æ—Ñ–∏—Å–µ',
        r'^—É–¥–∞–ª[–µ—ë]–Ω–Ω?—ã–π —Ñ–æ—Ä–º–∞—Ç',
        r'^–≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç',
        r'^–≥—Ä–∞—Ñ–∏–∫',
        r'^–∑–∞—Ä–ø–ª–∞—Ç–∞',
        r'^–∑/?–ø',
        r'^\d+/\d+',  # 5/2
        r'^–æ—Ñ–∏—Å',
        r'^–ª–æ–∫–∞—Ü–∏—è',
        r'^–¥–º—Å',
        r'^–≤–º—Å',
        r'–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤',
        r'—Ç–∏–º–±–∏–ª–¥–∏–Ω–≥',
        r'–±–µ—Å–ø–ª–∞—Ç–Ω[—ã–æ][–π–µ]',
        r'–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏[—è—é]',

        # –û –∫–æ–º–ø–∞–Ω–∏–∏
        r'^–æ –∫–æ–º–ø–∞–Ω–∏–∏',
        r'^–Ω–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è',
        r'^–Ω–∞—à–∞ —Ü–µ–ª—å',
        r'^–º—ã (?:—è–≤–ª—è–µ–º—Å—è|–∑–∞–Ω–∏–º–∞–µ–º—Å—è|—Å–æ–∑–¥–∞–µ–º)',

        # –ü—Ä–∏–∑—ã–≤—ã
        r'^–µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ',
        r'^–µ—Å–ª–∏ (?:—Ç–µ–±–µ|–≤–∞–º) –≤–∞–∂–Ω–æ',
        r'^–±—É–¥–µ–º —Ä–∞–¥—ã',
        r'^–∂–¥–µ–º',
        r'^–ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å',

        # –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
        r'^—É–Ω–∏–∫–∞–ª—å–Ω',
        r'^–æ—Ç–ª–∏—á–Ω',
        r'^–ø—Ä–µ–∫—Ä–∞—Å–Ω',
        r'^–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç',
        r'^–∫–∞—Ä—å–µ—Ä–Ω—ã–π —Ä–æ—Å—Ç',
        r'^–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å (?:—Ä–∞–∑–≤–∏—Ç–∏—è|—Ä–æ—Å—Ç–∞)',
        r'^–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã',
        r'^–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω[–∞—ã][—è–π] –∑–∞—Ä–ø–ª–∞—Ç',
        r'^–∫–æ–º—Ñ–æ—Ä—Ç–Ω[–∞—ã][—è–π]',
        r'^–¥—Ä—É–∂–Ω[–∞—ã][—è–π] –∫–æ–º–∞–Ω–¥',

        # –≠–º–æ–¥–∑–∏
        r'üì©|üìß|‚úâÔ∏è|üíº|üéØ|üöÄ|‚≠ê|‚ú®|üîç|üìä|üìà|üí∞|üèÜ',
    ]

    def __init__(
            self,
            min_length: int = 15,
            max_length: int = 300,
            min_words: int = 3,
            similarity_threshold: float = 0.85
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞."""
        self.min_length = min_length
        self.max_length = max_length
        self.min_words = min_words
        self.similarity_threshold = similarity_threshold
        self._compile_patterns()

    def _compile_patterns(self):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π."""
        self.header_pattern = re.compile(
            r'(?:^|\n)\s*(?:' + '|'.join(self.REQUIREMENT_HEADERS) + r')\s*(?:\n|$)',
            re.IGNORECASE | re.MULTILINE | re.UNICODE
        )
        self.marker_pattern = re.compile(
            r'\b(?:' + '|'.join(self.REQUIREMENT_MARKERS) + r')',
            re.IGNORECASE | re.UNICODE
        )
        self.stop_section_pattern = re.compile(
            r'(?:^|\n)\s*(?:' + '|'.join(self.STOP_SECTION_HEADERS) + r')',
            re.IGNORECASE | re.MULTILINE | re.UNICODE
        )
        self.noise_pattern = re.compile(
            '|'.join(self.NOISE_PHRASES),
            re.IGNORECASE | re.UNICODE
        )

    def extract(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return []

        requirements = []

        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º —Å–µ–∫—Ü–∏–π
        section_requirements = self._extract_from_sections(text)
        requirements.extend(section_requirements)

        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º –≤ —Ç–µ–∫—Å—Ç–µ
        marker_requirements = self._extract_by_markers(text)
        requirements.extend(marker_requirements)

        # –ú–µ—Ç–æ–¥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–æ–≤
        list_requirements = self._extract_from_lists(text)
        requirements.extend(list_requirements)

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        requirements = self._advanced_clean_and_deduplicate(requirements)

        return requirements

    def _extract_from_sections(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π."""
        requirements = []

        for match in self.header_pattern.finditer(text):
            section_start = match.end()
            section_end = self._find_section_end(text, section_start)

            if section_end is None:
                section_end = len(text)

            section_text = text[section_start:section_end]

            if not self.stop_section_pattern.search(match.group()):
                items = self._split_into_items(section_text)
                requirements.extend(items)

        return requirements

    def _find_section_end(self, text: str, start_pos: int) -> int:
        """–ü–æ–∏—Å–∫ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—É—â–µ–π —Å–µ–∫—Ü–∏–∏."""
        next_headers_pattern = re.compile(
            r'\n\s*(?:'
            r'–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|'
            r'–∑–∞–¥–∞—á–∏|'
            r'responsibilities|'
            r'—É—Å–ª–æ–≤–∏—è|'
            r'–º—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º|'
            r'what we offer|'
            r'benefits|'
            r'–æ –∫–æ–º–ø–∞–Ω–∏–∏|'
            r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è|'
            r'requirements'
            r'):',
            re.IGNORECASE | re.UNICODE
        )

        match = next_headers_pattern.search(text[start_pos:])

        if match:
            return start_pos + match.start()

        return len(text)

    def _extract_by_markers(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º –º–∞—Ä–∫–µ—Ä–∞–º."""
        requirements = []
        sentences = re.split(r'[.!?]\s+', text)

        for sentence in sentences:
            if self.marker_pattern.search(sentence):
                cleaned = sentence.strip()

                if self._is_valid_requirement(cleaned):
                    requirements.append(cleaned)

        return requirements

    def _extract_from_lists(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤."""
        requirements = []

        list_patterns = [
            r'^[-‚Ä¢*]\s*(.+)$',
            r'^\d+[\.)]\s*(.+)$',
        ]

        lines = text.split('\n')

        for line in lines:
            for pattern in list_patterns:
                match = re.match(pattern, line.strip())

                if match:
                    item = match.group(1).strip()

                    if self._is_valid_requirement(item):
                        requirements.append(item)
                    break

        return requirements

    def _split_into_items(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã."""
        items = []

        separators = [';', '\n']
        current_items = [text]

        for sep in separators:
            new_items = []
            for item in current_items:
                new_items.extend(item.split(sep))
            current_items = new_items

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–µ
        final_items = []
        for item in current_items:
            parts = re.split(r'\.\s+(?=[–ê-–ØA-Z–Å])', item)
            final_items.extend(parts)

        for item in final_items:
            cleaned = self._clean_item(item)

            if self._is_valid_requirement(cleaned):
                items.append(cleaned)

        return items

    def _clean_item(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        # –£–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏
        text = re.sub(r'[\U0001F300-\U0001F9FF]', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        text = re.sub(r'^[-‚Ä¢*\d+\.)]\s*', '', text)

        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = ' '.join(text.split())

        return text.strip()

    def _is_valid_requirement(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è."""
        if not text:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(text) < self.min_length or len(text) > self.max_length:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤
        words = text.split()
        if len(words) < self.min_words:
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        if text.strip().endswith(':'):
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑
        if self.noise_pattern.search(text):
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —á–∏—Å—Ç–æ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if re.match(r'^\d+[\s\-/]*\d*$', text.strip()):
            return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏—Ö —Ñ—Ä–∞–∑
        generic_phrases = [
            '–º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤',
            '–∏–¥–µ–∞–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç',
            '–≤—ã –Ω–∞–º –ø–æ–¥—Ö–æ–¥–∏—Ç–µ',
        ]

        text_lower = text.lower()
        for phrase in generic_phrases:
            if phrase in text_lower and len(text) < 100:
                return False

        return True

    def _advanced_clean_and_deduplicate(self, requirements: List[str]) -> List[str]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è."""
        if not requirements:
            return []

        cleaned = [req for req in requirements if self._is_valid_requirement(req)]

        unique_requirements = []

        for req in cleaned:
            is_duplicate = False

            for existing in unique_requirements:
                similarity = self._calculate_similarity(req, existing)

                if similarity >= self.similarity_threshold:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_requirements.append(req)

        return unique_requirements

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö —Å—Ç—Ä–æ–∫."""
        norm1 = ' '.join(text1.lower().split())
        norm2 = ' '.join(text2.lower().split())

        return SequenceMatcher(None, norm1, norm2).ratio()


class SkillsBasedRequirementsExtractor(RequirementsExtractor):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏."""

    def __init__(
            self,
            tech_keywords: List[str],
            min_length: int = 15,
            max_length: int = 300,
            min_words: int = 3,
            similarity_threshold: float = 0.85
    ):
        super().__init__(min_length, max_length, min_words, similarity_threshold)
        self.tech_keywords = [kw.lower() for kw in tech_keywords]

    def extract(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π."""
        base_requirements = super().extract(text)

        tech_requirements = []
        other_requirements = []

        for req in base_requirements:
            req_lower = req.lower()

            if any(kw in req_lower for kw in self.tech_keywords):
                tech_requirements.append(req)
            else:
                other_requirements.append(req)

        return tech_requirements + other_requirements

    def get_tech_requirements_only(self, text: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π."""
        all_requirements = super().extract(text)

        return [
            req for req in all_requirements
            if any(kw in req.lower() for kw in self.tech_keywords)
        ]
